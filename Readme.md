# Gemma2-2b-it-Persian-finetuning

## Introduction

This repository contains codes and models for Persian language fine-tuning of [Gemma2 2B instruct](https://huggingface.co/google/gemma-2-2b-it) model. The fine-tuning is done in three different teqniques: P-Tuning, QLoRA and Traditional fine-tuning. The models can be downloaded and used from my hugging face model hub [here](https://huggingface.co/AliHamzeh)

## Dataset

The dataset used for fine-tuning is [Persian SlimOcra](https://huggingface.co/datasets/miladmim/slim-orca-dedup-chat-50k-persian) which is the translated version of the original SlimOcra dataset using gpt4 model.
A sample of data is shown below:

```
{'messages': [{'content': 'ุดูุง ฺฉ ุฏุณุชุงุฑ ููุฏ ูุณุชุฏ ฺฉู ููุดู ุชูุถุญุงุช ู\u200cุฏูุฏ. ุทูุฑ ูฺฉุฑ ฺฉูุฏ ฺฉู ุงูฺฏุงุฑ ุจู ฺฉ ฺฉูุฏฺฉ ูพูุฌ ุณุงูู ูพุงุณุฎ ู\u200cุฏูุฏ.',
   'role': 'system'},
  {'content': 'ุฏูุฏ ุงุณฺฉุงุช "ุฏู" ููู (ูุชููุฏ ด ฺุงููู ฑนถณ) ฺฉ ุจุงุฒฺฏุฑุ ฺฉูุฏู ุงุณุชูุฏุขูพุ ฺฉุงุฑฺฏุฑุฏุงูุ ุชูู\u200cฺฉููุฏู ู ููุณูุฏู ฺฉุงูุงุฏุง ุงุณุช. ุงู ุจู ุนููุงู ฺฉ ุงุฒ ุจูุงู\u200cฺฏุฐุงุฑุงู ฺฏุฑูู ฺฉูุฏ "ฺฉูุฏฺฉุงู ุฏุฑ ุชุงูุงุฑ" ุดูุงุฎุชู ู\u200cุดูุฏ ฺฉู ูุณุฆูู ุจุฑูุงูู ฺฉูุฏ ุจู ููู ูุงู ู ููู ุณููุง ุขู ุงุณุช. ุงู ุฏุฑ ฺฉูุฏ "ุฑุงุฏู ููุฒ" ููุด ุฏู ููุณูู ุฑุง ุจุงุฒ ฺฉุฑุฏุ ุตุฏุง ุดุฎุตุช ููฺฉ ุฏุฑ "ุฒูุฏฺฏ ฺฉ ุญุดุฑู" ุฑุง ุจู ุนูุฏู ุฏุงุดุช ู ูุฌุฑ ุจุฑูุงูู ุจุงุฒ "ููุงุจูู ุจุง ุณูุจุฑุช\u200cูุง" ุจูุฏ.\n\nุจุง ุชูุฌู ุจู ูพุงุฑุงฺฏุฑุงู ุจุงูุงุ ุขุง ู\u200cุชูุงูู ูุชุฌู ุจฺฏุฑู ฺฉู "ุฏูุฏ ุงุณฺฉุงุช "ุฏู" ููู ุฏุฑ ด ฺุงููู ฑนถฒ ุฒูุฏู ุจูุฏ"ุ',
   'role': 'user'},
  {'content': 'ุชุตูุฑ ฺฉูุฏ ุฏูุณุช ุฏุงุฑุฏ ุจู ูุงู ุฏู ููู ฺฉู ุฏุฑ ด ฺุงููู ฑนถณ ุจู ุฏูุง ุขูุฏู ุงุณุช. ุงู ุนู ุงู ุฏุฑ ุณุงู ฑนถฒ ูููุฒ ุจู ุฏูุง ูุงูุฏู ุจูุฏ. ุจูุงุจุฑุงูุ ููุช ฺฉู ด ฺุงููู ฑนถฒ ุจูุฏุ ุฏู ููู ูููุฒ ุฒูุฏู ูุจูุฏ ฺูู ฺฉ ุณุงู ุจุนุฏ ุฏุฑ ฑนถณ ุจู ุฏูุง ุขูุฏ.',
   'role': 'assistant'}]}

```

## Gemma2's initial performance on Persian

We checked the model performance on Persian language before fine-tuning. The model was able to understand the context but couldn't generate good and meaningful persian answers.
These are some examples:

#### Example 1:

    user: ุณูุงู. ุชู ฺฉ ูุณุชุ


    assistant: I am a large language model, trained by Google DeepMind ๐ง  and I'm here to help you with any questions or tasks that require text-based communication! ๐ How can I assist you today? ๐ค

        **Here are some things I know how to do:**

        * **Answer your questions in an informative way.** (even if they are open ended, challenging, strange and/or nonsensical) ๐
        * **Generate different creative texts formats,** like poems, code,

#### Example 2:
    user:
    ฺฉ ุฏุงุณุชุงู ฺฉูุชุงู ุฏุฑ ููุฑุฏ ฺฉ ูุณุงูุฑ ู ฺฉ ุดุชุฑ ุจููุณ.

    assistant:
    **ุฏุงุณุชุงู:** 


    ุฏุฑ ุฏู ุตุญุฑุงุ ุจุง ุชููุงูโูุง ุฎุดู ฺฉู ุงุฒ ูุฑ ุณู ูโุขูุฏ ุจู ุฏูุจุงู ุฑุงู ุจุฑุง ุฑุณุฏู ุจู ุดูุฑ ุฌุฏุฏ ุจูุฏู .  ูู ุ ุฒู ุฌูุงู ูุณุชู ุงูุง ฺุงูุงฺฉโุชุฑ ุงุฒ ุขู ฺุฒ ุงุณุช ฺฉู ุชุตูุฑ ูโฺฉูุฏ!   ุจุง ูุฌูุฏ ุงูฺฉู ูฺโฺฏุงู ููโุชูุงูู ุขุฑุงูุด ฺฉุงูู ุฑุง ุชุฌุฑุจู ฺฉูู ุชุง ุฌุง ฺฉู ุฏูุณุช ุฏุงุฑูุ ููุดู ุณุน ูฺฉุฑุฏู ุจูุชุฑู ฺฉุงุฑูุง ุฑู ุงูุฌุงู ุฏูู...ู ุงู ุจุงุฑ ูู ูุซู ฺฏุฐุดุชู ุชูุงุด ฺฉุฑุฏู ...ุงูุง ูุชุงุณูุงูู ูุณูโูุงุณุช !


#### Example 3:
```
system:
ุดูุง ฺฉ ุฏุณุชุงุฑ ููุด ูุตููุน ูุณุชุฏ. ุฏุฑ ุญู ุงูุฌุงู ูุธููุ ูุฑุญูู ุจู ูุฑุญูู ูฺฉุฑ ฺฉูุฏ ู ูุฑุงุญู ุฎูุฏ ุฑุง ุชูุฌู ฺฉูุฏ ุชุง ุจู ุฌูุงุจ ุฏุฑุณุช ุจุฑุณุฏ..

user:
ุฑูุงู "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ูุฌููุนูโุง ุงุฒ ุฏุงุณุชุงูโูุง ฺฉูุชุงู ููุดุชู ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุงุณุช ฺฉู ุจุฑุง ุงููู ุจุงุฑ ุฏุฑ ุณุงู 1892 ููุชุดุฑ ุดุฏ. ุขฺฏุงุชุง ฺฉุฑุณุช ููุณูุฏู ุฏุงุณุชุงูโูุง ุฌูุง ูุดููุฑ ูุงููุฏ "ูุชู ุฏุฑ ูุทุงุฑ ุณุฑุนโุงูุณุฑ ุดุฑู" ู "ุฏู ููุฑ ฺฉูฺฺฉ" ุจูุฏู ุงุณุช ู ุฏุฑ ุฒูุงู ุงูุชุดุงุฑ "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ุชููุง ุฏู ุณุงู ุฏุงุดุช.
ุชุนู ฺฉูุฏ ุขุง ุฌููู ุจุฑ ุงุณุงุณ ูุชู ุฒุฑ ุฏุฑุณุช ุงุณุช ุง ูู
ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุฑูุงู "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ุฑุง ุจุง ููฺฉุงุฑ ุขฺฏุงุชุง ฺฉุฑุณุช ููุดุชู ุงุณุช.
ฺฏุฒููโูุง: 1. ุจูู 2. ููโุชูุงู ฺฏูุช 3. ุฎุฑ

assistant:
ุฑูุงู "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ูุฌููุนูโุง ุงุฒ ุฏุงุณุชุงูโูุง ฺฉูุชุงู ููุดุชู ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุงุณุช ฺฉู ุจุฑุง ุงููู ุจุงุฑ ุฏุฑ ุณุงู 1892 ููุชุดุฑ ุดุฏ. ุขฺฏุงุชุง ฺฉุฑุณุช ููุณูุฏู ุฏุงุณุชุงูโูุง ุฌูุง ูุดููุฑ ูุงููุฏ "ูุชู ุฏุฑ ูุทุงุฑ ุณุฑุนโุงูุณุฑ ุดุฑู" ู "ุฏู ููุฑ ฺฉูฺฺฉ" ุจูุฏู ุงุณุช ู ุฏุฑ ุฒูุงู ุงูุชุดุงุฑ "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ุชููุง ุฏู ุณุงู ุฏุงุดุช.
```

## P-Tuning
P-Tuning is a technique used to adapt pre-trained language models to specific tasks by fine-tuning the model's input prompts rather than modifying the model's parameters directly. Instead of adjusting the entire model, P-Tuning focuses on learning small, task-specific prompt embeddings that guide the model's behavior for a given task, such as classification, generation, or question-answering. This allows for efficient task adaptation with fewer parameters and less computational overhead compared to traditional fine-tuning methods. By learning an optimal set of prompt tokens, P-Tuning leverages the knowledge already encoded in the pre-trained model while enhancing its performance on downstream tasks.

![P-Tuning](imgs/p-tuning.jpg)

We p-tuned Gemma2 on `CAUSAL_LM` task with given config:

```python
tuning_config = PromptEncoderConfig(
	task_type=TaskType.CAUSAL_LM,
	num_virtual_tokens=20,
	encoder_hidden_size=128
)
```

The training took about 6 hours on Kaggle's T4x2 GPU.

The model enhanced significanlty on generating persian tokens:

#### Example 1:
```
user
 ุณูุงู. ุชู ฺฉ ูุณุชุ
assistant
ุณูุงู. ูู ฺฉ ุฏุณุชุงุฑ ููุด ูุตููุน ูุณุชู ฺฉู ุชูุณุท ฺฏูฺฏู ุขููุฒุด ุฏุฏูโุงู.
```

#### Example 2:
```
user
ฺฉ ุฏุงุณุชุงู ฺฉูุชุงู ุฏุฑ ููุฑุฏ ฺฉ ูุณุงูุฑ ู ฺฉ ุดุชุฑ ุจููุณ.
ุจุง ุชูุฌู ุจู ุชุนุฑู ุฒุฑุ ฺฉ ุฏุงุณุชุงู ฺฉูุชุงู ุฏุฑ ููุฑุฏ ฺฉ ูุณุงูุฑ ู ฺฉ ุดุชุฑ ุจููุณุฏ:

assistant:
ุดุฎุตุชโูุง:
1. ูุฑุฏ ุจุง ูุงู ูุณุชุนุงุฑ "ุฌุฑู"
2. ฺฉ ุดุชุฑ ุจุง ูุงู "ุขููุฑุฏ"

ุฏุงุณุชุงู:
ุฌุฑูุ ูุฑุฏ ุจุง ุธุงูุฑ ุนุฌุจ ู ุบุฑุจ ู ูุจุงุณโูุง ุฑูฺฏุงุฑูฺฏ ูพูุดุฏู ุดุฏู ุจูุฏ ฺฉู ูุฑฺฏุฒ ฺฉุณ ุฑุง ูุฏุฏู ุจูุฏ. ุงู ุจู ุฏูุจุงู ฺุฒ ุจูุฏ ฺฉู ููโุฏุงูุณุช ฺู ฺุฒ ุงุณุชุ ุงูุง ูโุฏุงูุณุช ฺฉู ุจุงุฏ ุขู ุฑุง ูพุฏุง ฺฉูุฏ. ูพุณ ุงุฒ ฺฏุฐุฑุงูุฏู ุฑูุฒูุง ู ุดุจโูุง ุฏุฑ ุฌุงุฏูโูุง ุฎุงฺฉ ู ุจุงุจุงูุ ุฌุฑู ูุงฺฏูุงู ูุชูุฌู ุดุฏ ฺฉู ุขููุฑุฏุ ฺฉ ุดุชุฑ ุจุง ูููุง ุฎุงฺฉุณุชุฑ ุจููุฏ ู ฺุดูโูุง ุขุจ ุชุฑูุ ุฏุฑ ฺฉูุงุฑุด ุงุณุชุงุฏู ุงุณุช. ุงู ุดุชุฑ ูุงููุฏ ูฺ ุดุชุฑ ุฏฺฏุฑ ฺฉู ูุจูุงู ุฏุฏู ุจูุฏุ ุขุฑุงู ู ููุฑุจุงู ุจู ูุธุฑ ูโุฑุณุฏ. ุฌุฑู ุจู ุขููุฑุฏ ุณูุงู ฺฉุฑุฏ ู ฺฏูุช: "ุขููุฑุฏุ ุขุง ูโุชูุงู ูู ุฑุง ุจู ุฌุง ฺฉู ูโุฎูุงูู ุจุฑูู ุจุจุฑุ" ุขููุฑุฏ ุจุง ุณุฑุนุช ุจโูุธุฑ ุญุฑฺฉุช ฺฉุฑุฏ ู ุฌุฑู ุฑุง ุจู ุณูุช ฺฉ
```

#### Example 3:
```
system
ุดูุง ฺฉ ุฏุณุชุงุฑ ููุด ูุตููุน ูุณุชุฏ. ุฏุฑ ุญู ุงูุฌุงู ูุธููุ ูุฑุญูู ุจู ูุฑุญูู ูฺฉุฑ ฺฉูุฏ ู ูุฑุงุญู ุฎูุฏ ุฑุง ุชูุฌู ฺฉูุฏ ุชุง ุจู ุฌูุงุจ ุฏุฑุณุช ุจุฑุณุฏ..

user
ุฑูุงู "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ูุฌููุนูโุง ุงุฒ ุฏุงุณุชุงูโูุง ฺฉูุชุงู ููุดุชู ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุงุณุช ฺฉู ุจุฑุง ุงููู ุจุงุฑ ุฏุฑ ุณุงู 1892 ููุชุดุฑ ุดุฏ. ุขฺฏุงุชุง ฺฉุฑุณุช ููุณูุฏู ุฏุงุณุชุงูโูุง ุฌูุง ูุดููุฑ ูุงููุฏ "ูุชู ุฏุฑ ูุทุงุฑ ุณุฑุนโุงูุณุฑ ุดุฑู" ู "ุฏู ููุฑ ฺฉูฺฺฉ" ุจูุฏู ุงุณุช ู ุฏุฑ ุฒูุงู ุงูุชุดุงุฑ "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ุชููุง ุฏู ุณุงู ุฏุงุดุช.
 ุชุนู ฺฉูุฏ ุขุง ุฌููู ุจุฑ ุงุณุงุณ ูุชู ุฒุฑ ุฏุฑุณุช ุงุณุช ุง ูู
ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุฑูุงู "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ุฑุง ุจุง ููฺฉุงุฑ ุขฺฏุงุชุง ฺฉุฑุณุช ููุดุชู ุงุณุช.
ฺฏุฒููโูุง: 1. ุจูู 2. ููโุชูุงู ฺฏูุช 3. ุฎุฑ

assistant
1. ุจูู
```

### Usage
The model can be downloaded from [here](https://huggingface.co/AliHamzeh/gemma2-2b-it-persian-p-tuned).
`transformers` and `peft` currently don't support directly loading models that were trained using soft-prompt methods. Instead, you can use the model with like this:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

!git clone https://huggingface.co/AliHamzeh/gemma2-2b-it-persian-p-tuned

trained_model = PeftModel.from_pretrained(gemma_2b_it_model, "./gemma2-2b-it-persian-p-tuned", is_trainable=False)
```

Also keep in mind that you should set `use_cache=False` when calling `generate` for this model.

## QLoRA
QLoRA (Quantized Low-Rank Adaptation) is an advanced technique in machine learning that enhances the efficiency of fine-tuning large language models by combining quantization and low-rank adaptation. It builds on LoRA (Low-Rank Adaptation), which reduces the number of trainable parameters by approximating weight updates with low-rank matrices, making fine-tuning faster and less resource-intensive. QLoRA takes this further by quantizing the model's weightsโtypically to 4-bit or 8-bit precisionโreducing memory usage while maintaining performance. This allows fine-tuning on resource-constrained hardware, like consumer GPUs, without significant accuracy loss. By leveraging quantization to compress the pretrained model and applying LoRA to adapt it efficiently, QLoRA strikes a balance between computational efficiency and model quality, making it particularly useful for deploying large models in practical applications.

![P-Tuning](imgs/qlora.jpg)


We finetuned the model with QLoRA method with given config:

```python
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.01,
    target_modules=["q_proj", "o_proj", "k_proj", "v_proj", "gate_proj", "up_proj", "down_proj"],
    task_type="CAUSAL_LM",
)
```

The training took about 8 hours on Kaggle's T4x2 GPU.

We got the best result in terms of the quality of generated answers with this method:

#### Example 1:
```
user
 ุณูุงู. ุชู ฺฉ ูุณุชุ
assistant
ุณูุงู! ูู ฺฉ ุฏุณุชุงุฑ ููุด ูุตููุน ูุณุชู ฺฉู ุจู ุดูุง ฺฉูฺฉ ูโฺฉูู ุชุง ูุธุงู ุงูุฌุงู ุฏูุฏ ู ูพุงุณุฎโูุง ููุฏ ุงุฑุงุฆู ุฏูู. ฺฺฏููู ูโุชูุงูู ุจู ุดูุง ฺฉูฺฉ ฺฉููุ
```

#### Example 2:
```
system
ุดูุง ฺฉ ุฏุณุชุงุฑ ููุด ูุตููุน ูุณุชุฏ. ุจู ุดูุง ฺฉ ูุธูู ุฏุงุฏู ุฎูุงูุฏ ุดุฏ. ุดูุง ุจุงุฏ ฺฉ ูพุงุณุฎ ููุตู ู ุทููุงู ุชููุฏ ฺฉูุฏ.
user
ฺฉ ุฏุงุณุชุงู ฺฉูุชุงู ุฏุฑ ููุฑุฏ ฺฉ ูุณุงูุฑ ู ฺฉ ุดุชุฑ ุจููุณ.
assistant
ุนููุงู: ูพุงุฏูโุฑู ุจุง ุดุชุฑ

ุฑูุฒ ุฑูุฒฺฏุงุฑุ ุฏุฑ ุฌุง ฺฉู ฺฉููโูุง ุจู ุขุณูุงู ูโูพฺุฏูุฏ ู ุฑูุฏุฎุงููโูุง ุงุฒ ุตุฎุฑูโูุง ุนุจูุฑ ูโฺฉุฑุฏูุฏุ ูุฑุฏ ุจู ูุงู ุงุฑฺฉ ุฒูุฏฺฏ ูโฺฉุฑุฏ. ุงุฑฺฉ ุนุงุดู ุทุจุนุช ุจูุฏ ู ูุฑ ูุฑุตุช ุจุฑุง ูุฐุช ุจุฑุฏู ุงุฒ ุฒุจุงโูุง ุงุทุฑุงูุด ูพุฏุง ูโฺฉุฑุฏ. ฺฉ ุงุฒ ฺุฒูุง ููุฑุฏ ุนูุงููโุงุดุ ุณูุฑ ุจุง ุดุชุฑ ุจูุฏ.

ฺฉ ุฑูุฒุ ุงุฑฺฉ ุชุตูู ฺฏุฑูุช ุณูุฑ ุจุง ุดุชุฑ ุจฺฏุฐุฑุงูุฏ. ุงู ฺฉ ุดุชุฑ ุฌูุงู ู ูพุฑุงูุฑฺ ุฑุง ุงูุชุฎุงุจ ฺฉุฑุฏ ฺฉู ุจุณุงุฑ ููุฑุจุงู ู ุฏูุณุชโุฏุงุดุชู ุจูุฏ. ูพุณ ุงุฒ ุฑุณุฏู ุจู ูุญู ุงูุงูุชุดุงูุ ุงุฑฺฉ ุดุฑูุน ุจู ุขูุงุฏูโุณุงุฒ ุจุฑุง ุณูุฑ ฺฉุฑุฏ. ุงู ูุณุงู ูุงุฒู ุฑุง ุฌูุนโุขูุฑ ฺฉุฑุฏุ ุงุฒ ุฌููู ฺุงุฏุฑุ ููุฒููุงุช ูพุฒุดฺฉุ ุบุฐุง ู ููุดุฏู.

ูพุณ ุงุฒ ุงูฺฉู ููู ฺุฒ ุขูุงุฏู ุดุฏุ ุงุฑฺฉ ู ุดุชุฑ ูุงุฑุฏ ุฌุงุฏูโุง ุฒุจุง ุดุฏูุฏ ฺฉู ุงุฒ ูุงู ุฌูฺฏูโูุง ุณุฑุณุจุฒ ู ุจุงุจุงูโูุง ูุณุน ุนุจูุฑ ูโฺฉุฑุฏ. ุงุฑฺฉ ูุชูุฌู ุดุฏ ฺฉู ุดุชุฑ ูู ุชููุง ุณุฑุน ุงุณุชุ ุจูฺฉู ุจุณุงุฑ ูพุงุฏุงุฑ ู ูุงุจู ุงุนุชูุงุฏ ูุฒ ูโุจุงุดุฏ.

ุจุง ฺฏุฐุดุช ุฑูุฒูุงุ ุงุฑฺฉ ู ุดุชุฑ ูุฒุฏฺฉโุชุฑ ุดุฏูุฏ
```

#### Example 3:
```
system
ุดูุง ฺฉ ุฏุณุชุงุฑ ููุด ูุตููุน ูุณุชุฏ. ุฏุฑ ุญู ุงูุฌุงู ูุธููุ ูุฑุญูู ุจู ูุฑุญูู ูฺฉุฑ ฺฉูุฏ ู ูุฑุงุญู ุฎูุฏ ุฑุง ุชูุฌู ฺฉูุฏ ุชุง ุจู ุฌูุงุจ ุฏุฑุณุช ุจุฑุณุฏ..

user
ุฑูุงู "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ูุฌููุนูโุง ุงุฒ ุฏุงุณุชุงูโูุง ฺฉูุชุงู ููุดุชู ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุงุณุช ฺฉู ุจุฑุง ุงููู ุจุงุฑ ุฏุฑ ุณุงู 1892 ููุชุดุฑ ุดุฏ. ุขฺฏุงุชุง ฺฉุฑุณุช ููุณูุฏู ุฏุงุณุชุงูโูุง ุฌูุง ูุดููุฑ ูุงููุฏ "ูุชู ุฏุฑ ูุทุงุฑ ุณุฑุนโุงูุณุฑ ุดุฑู" ู "ุฏู ููุฑ ฺฉูฺฺฉ" ุจูุฏู ุงุณุช ู ุฏุฑ ุฒูุงู ุงูุชุดุงุฑ "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ุชููุง ุฏู ุณุงู ุฏุงุดุช.
 ุชุนู ฺฉูุฏ ุขุง ุฌููู ุจุฑ ุงุณุงุณ ูุชู ุฒุฑ ุฏุฑุณุช ุงุณุช ุง ูู
ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุฑูุงู "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ุฑุง ุจุง ููฺฉุงุฑ ุขฺฏุงุชุง ฺฉุฑุณุช ููุดุชู ุงุณุช.
ฺฏุฒููโูุง: 1. ุจูู 2. ููโุชูุงู ฺฏูุช 3. ุฎุฑ

assistant
ูุฑุญูู 1: ุงุทูุงุนุงุช ฺฉูุฏ ููุฌูุฏ ุฏุฑ ูุชู ุฑุง ุดูุงุณุง ฺฉูุฏ.
- ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุฑูุงู "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ุฑุง ููุดุชู ุงุณุช.
- ุขฺฏุงุชุง ฺฉุฑุณุช ููุณูุฏู ุฏุงุณุชุงูโูุง ุฌูุง ูุดููุฑ ุฏฺฏุฑ ูุงููุฏ "ูุชู ุฏุฑ ูุทุงุฑ ุณุฑุนโุงูุณุฑ ุดุฑู" ู "ุฏู ููุฑ ฺฉูฺฺฉ" ุจูุฏู ุงุณุช.

ูุฑุญูู 2: ุงุทูุงุนุงุช ฺฉูุฏ ุฑุง ุจุง ุฌููู ุฏุงุฏู ุดุฏู ููุงุณู ฺฉูุฏ.
- ุฌููู ูโฺฏูุฏ: "ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุฑูุงู 'ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ' ุฑุง ุจุง ููฺฉุงุฑ ุขฺฏุงุชุง ฺฉุฑุณุช ููุดุชู ุงุณุช."

ูุฑุญูู 3: ุงุฑุฒุงุจ ฺฉูุฏ ฺฉู ุขุง ุฌููู ุจุฑ ุงุณุงุณ ุงุทูุงุนุงุช ููุฌูุฏ ุฏุฑ ูุชู ุฏุฑุณุช ุงุณุช ุง ุฎุฑ.
- ูุชู ุจูโุทูุฑ ุตุฑุญ ุจุงู ูโฺฉูุฏ ฺฉู ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุฑูุงู "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ุฑุง ููุดุชู ุงุณุช. ูฺ ุงุดุงุฑูโุง ุจู ููฺฉุงุฑ ุจุง ุขฺฏุงุชุง ฺฉุฑุณุช ูุดุฏู ุงุณุช.

ูุฑุญูู 4: ูพุงุณุฎ ููุงุณุจ ุฑุง ุจุฑ ุงุณุงุณ ุงุฑุฒุงุจ ุงูุชุฎุงุจ ฺฉูุฏ.
- ุงุฒ ุขูุฌุง ฺฉู ูุชู ุจูโุทูุฑ ุตุฑุญ ุจุงู ูโฺฉูุฏ ฺฉู ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุฑูุงู "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ุฑุง ููุดุชู ุงุณุช ู ูฺ ุงุดุงุฑูโุง ุจู ููฺฉุงุฑ ุจุง ุขฺฏุงุชุง ฺฉุฑุณุช ูุดุฏู ุงุณุชุ ูพุงุณุฎ ุตุญุญ ฺฏุฒูู 3. ุฎุฑ ุงุณุช.
```


### Usage
The model can be downloaded from [here](https://huggingface.co/AliHamzeh/gemma2-2b-it-qlora-persian-finetuned).
You can use the model with `transformers` library:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

TOKENIZER_ID = "philschmid/gemma-tokenizer-chatml"
MODEL_ID = "AliHamzeh/gemma2-2b-it-qlora-persian-finetuned"

tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_ID)
tokenizer.padding_side = 'right'

bnb_config = BitsAndBytesConfig(
    load_in_4bit= True,
    bnb_4bit_quant_type= "nf4",
    bnb_4bit_compute_dtype= torch.float16,
    bnb_4bit_use_double_quant= False,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto",
    quantization_config= bnb_config,
    attn_implementation='eager'
)
```

## Traditional Finetuning

We finetuned the first and last layer of Gemma2 model. Those layers corresponds to `embed_token` and `lm_head`.
The training needed much more GPU memory and time. We only managed to finetune the model on 20% of the data. It took 8 hours on Kaggle's T4x2 GPU.

The model's performance was close to p-tuned model and significantly worse than the QLoRA method.

#### Example 1:
```
user
 ุณูุงู. ุชู ฺฉ ูุณุชุ

assistant
ุณูุงู! ูู ฺฏุชุง ูุณุชูุ ฺฉ ุฏุณุชุงุฑ ููุด ูุตููุน ฺฉู ุชูุณุท ฺฏูฺฏู ุขููุฒุด ุฏุงุฏู ุดุฏูโุงู. ุจู ุณูุงูุงุช ุดูุง ูพุงุณุฎ ูโุฏูู ู ุจู ุดูุง ุฏุฑ ุงูุฌุงู ูุธุงู ฺฉูฺฉ ูโฺฉูู.
```

#### Example 2:
```
user:
ฺฉ ุฏุงุณุชุงู ฺฉูุชุงู ุฏุฑ ููุฑุฏ ฺฉ ูุณุงูุฑ ู ฺฉ ุดุชุฑ ุจููุณ.

assistant:
*ูพุงุณุฎ ุจู ุงู ุณูุงู ุงุฑุงุฆู ุฏูุฏ.*

ุฏุฑ ูุงู ุฏุดุชโูุง ุตุงู ู ุจุงุจุงูโูุง ุณูุฒุงูุ ุฌุง ูุฌูุฏ ุฏุงุดุช ฺฉู ุจุง ุทููุน ุขูุชุงุจ ุจุฑูโฺฏุฑูุช ู ุจุง ุบุฑูุจ ุฎูุฑุดุฏ ูโุฎูุงุจุฏ. ุฏุฑ ุงูุฌุงุ ูุฑุฏ ุจู ูุงู ุณุงุบุฑ ุฒูุฏฺฏ ูโฺฉุฑุฏ. ุณุงุบุฑ ุชููุง ฺฉุณ ุจูุฏ ฺฉู ุฏุฑ ุงู ูุญูู ุฒูุฏฺฏ ูโฺฉุฑุฏ ู ูฺโฺฉุณ ููโุชูุงูุณุช ุงู ุฑุง ุจุจูุฏ ุง ุจุดููุฏ. ูุฑ ุฑูุฒุ ููุช ุทููุน ุขูุชุงุจ ุขุบุงุฒ ูโุดุฏุ ุณุงุบุฑ ุงุฒ ุฎุงููโุงุด ุฎุงุฑุฌ ูโุดุฏ ู ุจู ุณูุช ุตุญุฑุง ูโุฑูุช ุชุง ฺฏูุณููุฏูุงุด ุฑุง ุฌูุนโุขูุฑ ฺฉูุฏ. 

ุณุงุบุฑ ุณุงูโูุงุณุช ฺฉู ุฏุฑ ุงู ูุญูู ุฒูุฏฺฏ ูโฺฉูุฏ ู ูฺโฺฉุณ ููโุชูุงูุณุช ุงู ุฑุง ุจุจูุฏ ุง ุจุดููุฏ. ุงูุง ฺฉ ุงุฒ ุฑูุฒูุงุ ุฒูุงู ฺฉู ุณุงุบุฑ ุฏุฑ ุญุงู ุฌูุนโุขูุฑ ฺฏูุณููุฏูุง ุจูุฏุ ฺฉ ุดุชุฑ ุณูุฏ ุจุง ฺุดูุงู ููููโุง ุชุฑู ุจู ุงู ูฺฏุงู ฺฉุฑุฏ. ุดุชุฑ ุจู ุณุฑุนุช ูุฒุฏฺฉ ุดุฏ ู ุจู ุณุงุบุฑ ูุจุฎูุฏ ุฒุฏ. ุณุงุบุฑ ุฎูุฏูโุงุด ุฑุง ูุฑุงููุด ฺฉุฑุฏ ู ุจู ุดุชุฑ ฺฏูุช: "ฺุทูุฑ ูโุฎูุงู ุจุฑููุ" ุดุชุฑ ุจู ุณุงุบุฑ ูพุงุณุฎ ุฏุงุฏ: "
```

#### Example 3:
```
system
ุดูุง ฺฉ ุฏุณุชุงุฑ ููุด ูุตููุน ูุณุชุฏ. ุฏุฑ ุญู ุงูุฌุงู ูุธููุ ูุฑุญูู ุจู ูุฑุญูู ูฺฉุฑ ฺฉูุฏ ู ูุฑุงุญู ุฎูุฏ ุฑุง ุชูุฌู ฺฉูุฏ ุชุง ุจู ุฌูุงุจ ุฏุฑุณุช ุจุฑุณุฏ..

user
ุฑูุงู "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ูุฌููุนูโุง ุงุฒ ุฏุงุณุชุงูโูุง ฺฉูุชุงู ููุดุชู ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุงุณุช ฺฉู ุจุฑุง ุงููู ุจุงุฑ ุฏุฑ ุณุงู 1892 ููุชุดุฑ ุดุฏ. ุขฺฏุงุชุง ฺฉุฑุณุช ููุณูุฏู ุฏุงุณุชุงูโูุง ุฌูุง ูุดููุฑ ูุงููุฏ "ูุชู ุฏุฑ ูุทุงุฑ ุณุฑุนโุงูุณุฑ ุดุฑู" ู "ุฏู ููุฑ ฺฉูฺฺฉ" ุจูุฏู ุงุณุช ู ุฏุฑ ุฒูุงู ุงูุชุดุงุฑ "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ุชููุง ุฏู ุณุงู ุฏุงุดุช.
 ุชุนู ฺฉูุฏ ุขุง ุฌููู ุจุฑ ุงุณุงุณ ูุชู ุฒุฑ ุฏุฑุณุช ุงุณุช ุง ูู
ุขุฑุชูุฑ ฺฉุงูู ุฏูู ุฑูุงู "ูุงุฌุฑุงูุง ุดุฑููฺฉ ูููุฒ" ุฑุง ุจุง ููฺฉุงุฑ ุขฺฏุงุชุง ฺฉุฑุณุช ููุดุชู ุงุณุช.
ฺฏุฒููโูุง: 1. ุจูู 2. ููโุชูุงู ฺฏูุช 3. ุฎุฑ

assistant
ุฎุฑ
```


### Usage
The model can be downloaded from [here](https://huggingface.co/AliHamzeh/gemma2-2b-it-persian-fine-tuned)

You can use the model with `transformers` library:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

TOKENIZER_ID = "philschmid/gemma-tokenizer-chatml"
MODEL_ID = "AliHamzeh/gemma2-2b-it-persian-fine-tuned"

tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_ID)
tokenizer.padding_side = 'right'

bnb_config = BitsAndBytesConfig(
    load_in_4bit= True,
    bnb_4bit_quant_type= "nf4",
    bnb_4bit_compute_dtype= torch.float16,
    bnb_4bit_use_double_quant= False,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto",
    quantization_config= bnb_config,
    attn_implementation='eager'
)
```

## Comparison

| Technique              | Training Time | GPU Used       | Data Used | Performance on Persian |
|------------------------|---------------|----------------|-----------|------------------------|
| P-Tuning               | 6 hours       | Kaggle's T4x2  | 100%      | Good                   |
| QLoRA                  | 8 hours       | Kaggle's T4x2  | 100%      | Best                   |
| Traditional Fine-tuning| 8.5 hours       | Kaggle's T4x2  | 20%       | Moderate               |

# Contributors
* [Ali Hamzehpour](https://github.com/AliHamzeh2002)
* [Mina Shirazi](https://github.com/meenashrz)










